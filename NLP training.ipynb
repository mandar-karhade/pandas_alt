{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08e5c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# specify GPU if you have a GPU \n",
    "device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb50308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lead dataset \n",
    "df = pd.read_csv(\"train.csv\")\n",
    "# mydata.csv has only 2 columns \"text\" and \"label\" (binary)\n",
    "df = df.sample(16000)[['comment_text','toxic']]\n",
    "df.columns = ['text','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ccf2ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18707</th>\n",
       "      <td>\"\\n\\n-\"\"Supernatural\"\" has an incorrect pronou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152781</th>\n",
       "      <td>\"\\nAboriginal Australians for one.  ☎ \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66073</th>\n",
       "      <td>While this may have elements of a content disp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69346</th>\n",
       "      <td>\"\\n\\nI have, to my knowledge, never disallowed...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76438</th>\n",
       "      <td>You cocksucker\\n\\nFUCK YOU OK? I hate you! Sto...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "18707   \"\\n\\n-\"\"Supernatural\"\" has an incorrect pronou...      0\n",
       "152781            \"\\nAboriginal Australians for one.  ☎ \"      0\n",
       "66073   While this may have elements of a content disp...      0\n",
       "69346   \"\\n\\nI have, to my knowledge, never disallowed...      0\n",
       "76438   You cocksucker\\n\\nFUCK YOU OK? I hate you! Sto...      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2843574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.907125\n",
      "1    0.092875\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check for class ditribution\n",
    "print(df['label'].value_counts(normalize = True))\n",
    "\n",
    "\n",
    "# Split data into train and test \n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
    "                                                                    random_state=1234, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "# Using temp_text and temp_labels created in previous step generate validation and test set\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=1234, \n",
    "                                                                test_size=0.1, \n",
    "                                                                stratify=temp_labels)\n",
    "# This will make 70% training data, 15% validation, and 15% test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a45ac645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape (11200,) , val data shape (4320,) ,test data shape (480,)\n"
     ]
    }
   ],
   "source": [
    "print(f'train data shape {train_text.shape} , val data shape {val_text.shape} ,test data shape {test_text.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4791c6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/m/.cache/pypoetry/virtualenvs/pandas-alt-gDH6xVLf-py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Import BERT tokenizer from the pretained model \n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Get length of all the sentences in the data\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "pd.Series(seq_len).hist(bins = 100)\n",
    "\n",
    "# Select the max_seq_len that we will be keeping based on the distribution above - target 85-90th percentile of text \n",
    "max_seq_len = 100\n",
    "\n",
    "# Tokenize sequences trimmed at max length determined above in the training, validation and testing sets\n",
    "# tokenize and encode sequences in the \n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# Convert tokens to tensors for train, validation, and test sts\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0df06eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader to prepare data for traiing\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 128\n",
    "\n",
    "# wrap tensors, random smapler, and prep dataloader for train data \n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors, random smapler, and prep dataloader for vaidation data \n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
    "\n",
    "# freeze all the parameters by setting requires_grad = False \n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e7968e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5511811  5.38461538]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m/.cache/pypoetry/virtualenvs/pandas-alt-gDH6xVLf-py3.10/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture\n",
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "      super(BERT_Arch, self).__init__()\n",
    "\n",
    "      self.bert = bert \n",
    "      \n",
    "      # dropout layer\n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "      # relu activation function\n",
    "      self.relu =  nn.ReLU()\n",
    "\n",
    "      # dense layer 1\n",
    "      self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "      # dense layer 2 (Output layer)\n",
    "      self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "      #softmax activation function\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "      x = self.fc1(cls_hs)\n",
    "\n",
    "      x = self.relu(x)\n",
    "\n",
    "      x = self.dropout(x)\n",
    "\n",
    "      # output layer\n",
    "      x = self.fc2(x)\n",
    "      \n",
    "      # apply softmax activation\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x\n",
    "\n",
    "\n",
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)\n",
    "\n",
    "\n",
    "\n",
    "# Find class weights and push those to GPU\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(train_labels),\n",
    "                                        y = train_labels                                                    \n",
    "                                    )\n",
    "# class_weights = dict(zip(np.unique(train_labels), class_weights))\n",
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_weights, dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "\n",
    "# define loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 100\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd3ff9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "  model.train()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "  total_preds=[]\n",
    "  \n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(train_dataloader):\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [r.to(device) for r in batch]\n",
    " \n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # clear previously calculated gradients \n",
    "    model.zero_grad()        \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "    preds = model(sent_id, mask)\n",
    "\n",
    "    # compute the loss between actual and predicted values\n",
    "    loss = cross_entropy(preds, labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "  # compute the training loss of the epoch\n",
    "  avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  #returns the loss and predictions\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "edf110b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "  print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "  total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(val_dataloader):\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "      preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "      loss = cross_entropy(preds,labels)\n",
    "\n",
    "      total_loss = total_loss + loss.item()\n",
    "\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "\n",
    "      total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "  avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d215d3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n",
      "Epoch 1/100\tTraining Loss: 0.573\tValidation Loss: 0.427\n",
      "\n",
      "Evaluating...\n",
      "Epoch 2/100\tTraining Loss: 0.417\tValidation Loss: 0.358\n",
      "\n",
      "Evaluating...\n",
      "Epoch 3/100\tTraining Loss: 0.394\tValidation Loss: 0.498\n",
      "\n",
      "Evaluating...\n",
      "Epoch 4/100\tTraining Loss: 0.408\tValidation Loss: 0.359\n",
      "\n",
      "Evaluating...\n",
      "Epoch 5/100\tTraining Loss: 0.386\tValidation Loss: 0.608\n",
      "\n",
      "Evaluating...\n",
      "Epoch 6/100\tTraining Loss: 0.398\tValidation Loss: 0.341\n",
      "\n",
      "Evaluating...\n",
      "Epoch 7/100\tTraining Loss: 0.373\tValidation Loss: 0.333\n",
      "\n",
      "Evaluating...\n",
      "Epoch 8/100\tTraining Loss: 0.349\tValidation Loss: 0.330\n",
      "\n",
      "Evaluating...\n",
      "Epoch 9/100\tTraining Loss: 0.454\tValidation Loss: 0.381\n",
      "\n",
      "Evaluating...\n",
      "Epoch 10/100\tTraining Loss: 0.379\tValidation Loss: 0.523\n",
      "\n",
      "Evaluating...\n",
      "Epoch 11/100\tTraining Loss: 0.381\tValidation Loss: 0.321\n",
      "\n",
      "Evaluating...\n",
      "Epoch 12/100\tTraining Loss: 0.345\tValidation Loss: 0.572\n",
      "\n",
      "Evaluating...\n",
      "Epoch 13/100\tTraining Loss: 0.357\tValidation Loss: 0.377\n",
      "\n",
      "Evaluating...\n",
      "Epoch 14/100\tTraining Loss: 0.351\tValidation Loss: 0.415\n",
      "\n",
      "Evaluating...\n",
      "Epoch 15/100\tTraining Loss: 0.405\tValidation Loss: 0.389\n",
      "\n",
      "Evaluating...\n",
      "Epoch 16/100\tTraining Loss: 0.359\tValidation Loss: 0.328\n",
      "\n",
      "Evaluating...\n",
      "Epoch 17/100\tTraining Loss: 0.347\tValidation Loss: 0.352\n",
      "\n",
      "Evaluating...\n",
      "Epoch 18/100\tTraining Loss: 0.367\tValidation Loss: 0.350\n",
      "\n",
      "Evaluating...\n",
      "Epoch 19/100\tTraining Loss: 0.332\tValidation Loss: 0.313\n",
      "\n",
      "Evaluating...\n",
      "Epoch 20/100\tTraining Loss: 0.335\tValidation Loss: 0.399\n",
      "\n",
      "Evaluating...\n",
      "Epoch 21/100\tTraining Loss: 0.360\tValidation Loss: 0.510\n",
      "\n",
      "Evaluating...\n",
      "Epoch 22/100\tTraining Loss: 0.342\tValidation Loss: 0.354\n",
      "\n",
      "Evaluating...\n",
      "Epoch 23/100\tTraining Loss: 0.354\tValidation Loss: 0.337\n",
      "\n",
      "Evaluating...\n",
      "Epoch 24/100\tTraining Loss: 0.330\tValidation Loss: 0.402\n",
      "\n",
      "Evaluating...\n",
      "Epoch 25/100\tTraining Loss: 0.335\tValidation Loss: 0.326\n",
      "\n",
      "Evaluating...\n",
      "Epoch 26/100\tTraining Loss: 0.339\tValidation Loss: 0.313\n",
      "\n",
      "Evaluating...\n",
      "Epoch 27/100\tTraining Loss: 0.317\tValidation Loss: 0.332\n",
      "\n",
      "Evaluating...\n",
      "Epoch 28/100\tTraining Loss: 0.346\tValidation Loss: 0.345\n",
      "\n",
      "Evaluating...\n",
      "Epoch 29/100\tTraining Loss: 0.348\tValidation Loss: 0.395\n",
      "\n",
      "Evaluating...\n",
      "Epoch 30/100\tTraining Loss: 0.346\tValidation Loss: 0.344\n",
      "\n",
      "Evaluating...\n",
      "Epoch 31/100\tTraining Loss: 0.354\tValidation Loss: 0.389\n",
      "\n",
      "Evaluating...\n",
      "Epoch 32/100\tTraining Loss: 0.365\tValidation Loss: 0.320\n",
      "\n",
      "Evaluating...\n",
      "Epoch 33/100\tTraining Loss: 0.318\tValidation Loss: 0.317\n",
      "\n",
      "Evaluating...\n",
      "Epoch 34/100\tTraining Loss: 0.325\tValidation Loss: 0.344\n",
      "\n",
      "Evaluating...\n",
      "Epoch 35/100\tTraining Loss: 0.320\tValidation Loss: 0.347\n",
      "\n",
      "Evaluating...\n",
      "Epoch 36/100\tTraining Loss: 0.350\tValidation Loss: 0.307\n",
      "\n",
      "Evaluating...\n",
      "Epoch 37/100\tTraining Loss: 0.324\tValidation Loss: 0.332\n",
      "\n",
      "Evaluating...\n",
      "Epoch 38/100\tTraining Loss: 0.323\tValidation Loss: 0.311\n",
      "\n",
      "Evaluating...\n",
      "Epoch 39/100\tTraining Loss: 0.328\tValidation Loss: 0.328\n",
      "\n",
      "Evaluating...\n",
      "Epoch 40/100\tTraining Loss: 0.321\tValidation Loss: 0.315\n",
      "\n",
      "Evaluating...\n",
      "Epoch 41/100\tTraining Loss: 0.325\tValidation Loss: 0.307\n",
      "\n",
      "Evaluating...\n",
      "Epoch 42/100\tTraining Loss: 0.323\tValidation Loss: 0.321\n",
      "\n",
      "Evaluating...\n",
      "Epoch 43/100\tTraining Loss: 0.330\tValidation Loss: 0.343\n",
      "\n",
      "Evaluating...\n",
      "Epoch 44/100\tTraining Loss: 0.340\tValidation Loss: 0.327\n",
      "\n",
      "Evaluating...\n",
      "Epoch 45/100\tTraining Loss: 0.327\tValidation Loss: 0.310\n",
      "\n",
      "Evaluating...\n",
      "Epoch 46/100\tTraining Loss: 0.336\tValidation Loss: 0.309\n",
      "\n",
      "Evaluating...\n",
      "Epoch 47/100\tTraining Loss: 0.330\tValidation Loss: 0.494\n",
      "\n",
      "Evaluating...\n",
      "Epoch 48/100\tTraining Loss: 0.319\tValidation Loss: 0.317\n",
      "\n",
      "Evaluating...\n",
      "Epoch 49/100\tTraining Loss: 0.338\tValidation Loss: 0.302\n",
      "\n",
      "Evaluating...\n",
      "Epoch 50/100\tTraining Loss: 0.330\tValidation Loss: 0.317\n",
      "\n",
      "Evaluating...\n",
      "Epoch 51/100\tTraining Loss: 0.333\tValidation Loss: 0.330\n",
      "\n",
      "Evaluating...\n",
      "Epoch 52/100\tTraining Loss: 0.321\tValidation Loss: 0.300\n",
      "\n",
      "Evaluating...\n",
      "Epoch 53/100\tTraining Loss: 0.342\tValidation Loss: 0.355\n",
      "\n",
      "Evaluating...\n",
      "Epoch 54/100\tTraining Loss: 0.323\tValidation Loss: 0.309\n",
      "\n",
      "Evaluating...\n",
      "Epoch 55/100\tTraining Loss: 0.305\tValidation Loss: 0.312\n",
      "\n",
      "Evaluating...\n",
      "Epoch 56/100\tTraining Loss: 0.319\tValidation Loss: 0.306\n",
      "\n",
      "Evaluating...\n",
      "Epoch 57/100\tTraining Loss: 0.325\tValidation Loss: 0.320\n",
      "\n",
      "Evaluating...\n",
      "Epoch 58/100\tTraining Loss: 0.311\tValidation Loss: 0.336\n",
      "\n",
      "Evaluating...\n",
      "Epoch 59/100\tTraining Loss: 0.309\tValidation Loss: 0.302\n",
      "\n",
      "Evaluating...\n",
      "Epoch 60/100\tTraining Loss: 0.309\tValidation Loss: 0.304\n",
      "\n",
      "Evaluating...\n",
      "Epoch 61/100\tTraining Loss: 0.316\tValidation Loss: 0.296\n",
      "\n",
      "Evaluating...\n",
      "Epoch 62/100\tTraining Loss: 0.311\tValidation Loss: 0.300\n",
      "\n",
      "Evaluating...\n",
      "Epoch 63/100\tTraining Loss: 0.320\tValidation Loss: 0.295\n",
      "\n",
      "Evaluating...\n",
      "Epoch 64/100\tTraining Loss: 0.321\tValidation Loss: 0.296\n",
      "\n",
      "Evaluating...\n",
      "Epoch 67/100\tTraining Loss: 0.319\tValidation Loss: 0.300\n",
      "\n",
      "Evaluating...\n",
      "Epoch 68/100\tTraining Loss: 0.321\tValidation Loss: 0.298\n",
      "\n",
      "Evaluating...\n",
      "Epoch 69/100\tTraining Loss: 0.332\tValidation Loss: 0.300\n",
      "\n",
      "Evaluating...\n",
      "Epoch 70/100\tTraining Loss: 0.308\tValidation Loss: 0.387\n",
      "\n",
      "Evaluating...\n",
      "Epoch 71/100\tTraining Loss: 0.321\tValidation Loss: 0.549\n",
      "\n",
      "Evaluating...\n",
      "Epoch 72/100\tTraining Loss: 0.319\tValidation Loss: 0.300\n",
      "\n",
      "Evaluating...\n",
      "Epoch 73/100\tTraining Loss: 0.310\tValidation Loss: 0.297\n",
      "\n",
      "Evaluating...\n",
      "Epoch 74/100\tTraining Loss: 0.310\tValidation Loss: 0.349\n",
      "\n",
      "Evaluating...\n",
      "Epoch 75/100\tTraining Loss: 0.312\tValidation Loss: 0.302\n",
      "\n",
      "Evaluating...\n",
      "Epoch 76/100\tTraining Loss: 0.305\tValidation Loss: 0.300\n",
      "\n",
      "Evaluating...\n",
      "Epoch 77/100\tTraining Loss: 0.316\tValidation Loss: 0.315\n",
      "\n",
      "Evaluating...\n",
      "Epoch 78/100\tTraining Loss: 0.314\tValidation Loss: 0.298\n",
      "\n",
      "Evaluating...\n",
      "Epoch 79/100\tTraining Loss: 0.312\tValidation Loss: 0.295\n",
      "\n",
      "Evaluating...\n",
      "Epoch 80/100\tTraining Loss: 0.303\tValidation Loss: 0.301\n",
      "\n",
      "Evaluating...\n",
      "Epoch 81/100\tTraining Loss: 0.302\tValidation Loss: 0.301\n",
      "\n",
      "Evaluating...\n",
      "Epoch 82/100\tTraining Loss: 0.308\tValidation Loss: 0.305\n",
      "\n",
      "Evaluating...\n",
      "Epoch 83/100\tTraining Loss: 0.306\tValidation Loss: 0.291\n",
      "\n",
      "Evaluating...\n",
      "Epoch 84/100\tTraining Loss: 0.312\tValidation Loss: 0.321\n",
      "\n",
      "Evaluating...\n",
      "Epoch 85/100\tTraining Loss: 0.307\tValidation Loss: 0.317\n",
      "\n",
      "Evaluating...\n",
      "Epoch 86/100\tTraining Loss: 0.297\tValidation Loss: 0.300\n",
      "\n",
      "Evaluating...\n",
      "Epoch 87/100\tTraining Loss: 0.303\tValidation Loss: 0.304\n",
      "\n",
      "Evaluating...\n",
      "Epoch 88/100\tTraining Loss: 0.323\tValidation Loss: 0.295\n",
      "\n",
      "Evaluating...\n",
      "Epoch 89/100\tTraining Loss: 0.304\tValidation Loss: 0.372\n",
      "\n",
      "Evaluating...\n",
      "Epoch 90/100\tTraining Loss: 0.316\tValidation Loss: 0.295\n",
      "\n",
      "Evaluating...\n",
      "Epoch 91/100\tTraining Loss: 0.310\tValidation Loss: 0.444\n",
      "\n",
      "Evaluating...\n",
      "Epoch 92/100\tTraining Loss: 0.310\tValidation Loss: 0.316\n",
      "\n",
      "Evaluating...\n",
      "Epoch 93/100\tTraining Loss: 0.320\tValidation Loss: 0.348\n",
      "\n",
      "Evaluating...\n",
      "Epoch 94/100\tTraining Loss: 0.313\tValidation Loss: 0.293\n",
      "\n",
      "Evaluating...\n",
      "Epoch 95/100\tTraining Loss: 0.304\tValidation Loss: 0.316\n",
      "\n",
      "Evaluating...\n",
      "Epoch 96/100\tTraining Loss: 0.303\tValidation Loss: 0.303\n",
      "\n",
      "Evaluating...\n",
      "Epoch 97/100\tTraining Loss: 0.311\tValidation Loss: 0.303\n",
      "\n",
      "Evaluating...\n",
      "Epoch 98/100\tTraining Loss: 0.299\tValidation Loss: 0.311\n",
      "\n",
      "Evaluating...\n",
      "Epoch 99/100\tTraining Loss: 0.316\tValidation Loss: 0.290\n",
      "\n",
      "Evaluating...\n",
      "Epoch 100/100\tTraining Loss: 0.296\tValidation Loss: 0.308\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "         \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}\\tTraining Loss: {train_loss:.3f}\\tValidation Loss: {valid_loss:.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e66bc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94       435\n",
      "           1       0.45      0.82      0.58        45\n",
      "\n",
      "    accuracy                           0.89       480\n",
      "   macro avg       0.72      0.86      0.76       480\n",
      "weighted avg       0.93      0.89      0.90       480\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>390</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    0   1\n",
       "row_0         \n",
       "0      390  45\n",
       "1        8  37"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "  preds = model(test_seq.to(device), test_mask.to(device))\n",
    "  preds = preds.detach().cpu().numpy()\n",
    "  \n",
    "  \n",
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))\n",
    "pd.crosstab(test_y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abc1d066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  432888 KB |    7460 MB |  126546 MB |  126124 MB |\n",
      "|       from large pool |  432384 KB |    7459 MB |  126348 MB |  125925 MB |\n",
      "|       from small pool |     504 KB |       3 MB |     198 MB |     198 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  432888 KB |    7460 MB |  126546 MB |  126124 MB |\n",
      "|       from large pool |  432384 KB |    7459 MB |  126348 MB |  125925 MB |\n",
      "|       from small pool |     504 KB |       3 MB |     198 MB |     198 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    7510 MB |    7510 MB |   12978 MB |    5468 MB |\n",
      "|       from large pool |    7506 MB |    7506 MB |   12972 MB |    5466 MB |\n",
      "|       from small pool |       4 MB |       6 MB |       6 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   50439 KB |    3871 MB |  241762 MB |  241712 MB |\n",
      "|       from large pool |   48896 KB |    3869 MB |  241558 MB |  241510 MB |\n",
      "|       from small pool |    1543 KB |       3 MB |     204 MB |     202 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     210    |     414    |    2062    |    1852    |\n",
      "|       from large pool |      77    |     153    |     984    |     907    |\n",
      "|       from small pool |     133    |     261    |    1078    |     945    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     210    |     414    |    2062    |    1852    |\n",
      "|       from large pool |      77    |     153    |     984    |     907    |\n",
      "|       from small pool |     133    |     261    |    1078    |     945    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      26    |      50    |      52    |      26    |\n",
      "|       from large pool |      24    |      47    |      49    |      25    |\n",
      "|       from small pool |       2    |       3    |       3    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      21    |      37    |     663    |     642    |\n",
      "|       from large pool |      17    |      35    |     432    |     415    |\n",
      "|       from small pool |       4    |       7    |     231    |     227    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch as tc\n",
    "print(tc.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717eae0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
